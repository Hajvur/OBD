close all;
clear all;
clc
% dane do sieci

x1 = [0 0 1 1];
x2 = [0 1 0 1];
y = [0 1 1 0];

x = [x1; x2];

% zalozenia sieci
epochs = 100;
learning_rate = 0.01;
tolerance = 0.0000001;
lambda = 0.01;
% liczba neuronow w warstwie wejsciowej, ukrytej i wyjsciowej
H1 = 3;
H2 = 2;
R1 = 2;
R2 = 1;
% 2 neurony wejsciowe
% 1 wyjsciowy
% 2 warstwy ukryte, 3 i 2 neurony
w1 = randn(H1, R1) * 0.1;
w2 = randn(H2, H1) * 0.1;
w3 = randn(R2, H2) * 0.1;
b1 = randn(H1, 1) * 0.1;
b2 = randn(H2, 1) * 0.1;
b3 = randn(R2, 1) * 0.1;

% funkcje pomocnicze

function [y] = sigmoid(x)
    y = 1 ./ (1 + exp(-x));
end

function [y] = Re_LU(x)
    y = max(0, x);
end

function [mse_ex] = mse(error)
    mse_ex = mean(error .^ 2);
end

function [y_predict] = forwardpass(w1, w2, w3, b1, b2, b3, x, y, epochs, tolerance, learning_rate, lambda)
    %forward pass
    for epoch = 1:epochs
        z1 = w1 * x + b1;
        a1 = sigmoid(z1);
        z2 = w2 * a1 + b2;
        a2 = sigmoid(z2);
        z3 = w3 * a2 + b3;
        y_predict = sigmoid(z3);
        % obliczanie bledu
        error = y_predict - y;
        disp(error)
        mse_ex = mse(error);
        disp("mse ex");
        disp(mse_ex)

        if mse_ex < tolerance
            break
        end

        %backpropagation (póki co tylko testy na pochodnych sigmoid)
        % pochodna sigmoid sigmoid(z3) * (1-sigmoid(z3)) albo podstawiamy od razu y_predict
        [gw1, gw2, gw3, gb1, gb2, gb3] = compute_gradient(error, w2, w3, a1, a2, x, y_predict);

        % aktualizacja wag i biasow
        w1 = w1 - learning_rate * (gw1 + lambda * w1); 
        w2 = w2 - learning_rate * (gw2 + lambda * w2);
        w3 = w3 - learning_rate * (gw3 + lambda * w3);
        
        % Aktualizacja biasów BEZ regularyzacji
        b1 = b1 - learning_rate * gb1; 
        b2 = b2 - learning_rate * gb2;
        b3 = b3 - learning_rate * gb3;
        disp("w3")
        disp(w3);
        disp("w2")
        disp(w2);
        disp("w1")
        disp(w1);
    end

end

function [gw1, gw2, gw3, gb1, gb2, gb3] = compute_gradient(error, w2, w3, a1, a2, x, y_predict)

    dw3 = error .* y_predict .* (1 - y_predict);
    dw2 = (w3' * dw3) .* a2 .* (1 - a2);
    dw1 = (w2' * dw2) .* a1 .* (1 - a1);

    gw3 = dw3 * a2';
    gw2 = dw2 * a1';
    gw1 = dw1 * x';

    gb3 = sum(dw3, 2);
    gb2 = sum(dw2, 2);
    gb1 = sum(dw1, 2);

end

function [] = compute_jacobian()

end

function [] = compute_hessian()

end

function [] = OBD()
end

y_predict = forwardpass(w1, w2, w3, b1, b2, b3, x, y, epochs, tolerance, learning_rate, lambda);
disp('Predykcja:')
disp(y_predict)
